{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5H9gvYjEDnwq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cK7-6OziLeIr"
   },
   "outputs": [],
   "source": [
    "file_path = os.path.join('.','final_cloth_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "znDuMIJnDDhP"
   },
   "outputs": [],
   "source": [
    "cloth_img=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oFL_6qKQLnRx"
   },
   "outputs": [],
   "source": [
    "\n",
    "footwear_img=pd.read_csv('final_footwear_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E9dd3xSlYTC8"
   },
   "outputs": [],
   "source": [
    "folder_path=os.path.join('.','images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_h50ydpbUHiO"
   },
   "outputs": [],
   "source": [
    "imgs_df=pd.concat([cloth_img,footwear_img],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "8Wcuz9_kUR86",
    "outputId": "308fbaff-b396-4b3d-c4b9-c60144a9950b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24607</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>24607.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13259</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>13259.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7188</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>7188.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7709</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>7709.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50942</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>50942.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>1786</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>1786.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>15719</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>15719.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>23929</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>23929.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>5393</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>5393.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>22716</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Sports Shoes</td>\n",
       "      <td>22716.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id masterCategory subCategory   articleType   filename\n",
       "0    24607        Apparel  Bottomwear         Jeans  24607.jpg\n",
       "1    13259        Apparel  Bottomwear         Jeans  13259.jpg\n",
       "2     7188        Apparel  Bottomwear         Jeans   7188.jpg\n",
       "3     7709        Apparel  Bottomwear         Jeans   7709.jpg\n",
       "4    50942        Apparel  Bottomwear         Jeans  50942.jpg\n",
       "..     ...            ...         ...           ...        ...\n",
       "832   1786       Footwear       Shoes  Sports Shoes   1786.jpg\n",
       "833  15719       Footwear       Shoes  Sports Shoes  15719.jpg\n",
       "834  23929       Footwear       Shoes  Sports Shoes  23929.jpg\n",
       "835   5393       Footwear       Shoes  Sports Shoes   5393.jpg\n",
       "836  22716       Footwear       Shoes  Sports Shoes  22716.jpg\n",
       "\n",
       "[837 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BiNduso3UvUz"
   },
   "outputs": [],
   "source": [
    "imgs_df['label']=imgs_df['masterCategory']+'_'+imgs_df['subCategory']+'_'+imgs_df['articleType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BP2YGkhpVGie",
    "outputId": "3ea2cca7-caf6-4997-e63f-920781321346"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24607</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>24607.jpg</td>\n",
       "      <td>Apparel_Bottomwear_Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13259</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>13259.jpg</td>\n",
       "      <td>Apparel_Bottomwear_Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7188</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>7188.jpg</td>\n",
       "      <td>Apparel_Bottomwear_Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7709</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>7709.jpg</td>\n",
       "      <td>Apparel_Bottomwear_Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50942</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>50942.jpg</td>\n",
       "      <td>Apparel_Bottomwear_Jeans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id masterCategory subCategory articleType   filename  \\\n",
       "0  24607        Apparel  Bottomwear       Jeans  24607.jpg   \n",
       "1  13259        Apparel  Bottomwear       Jeans  13259.jpg   \n",
       "2   7188        Apparel  Bottomwear       Jeans   7188.jpg   \n",
       "3   7709        Apparel  Bottomwear       Jeans   7709.jpg   \n",
       "4  50942        Apparel  Bottomwear       Jeans  50942.jpg   \n",
       "\n",
       "                      label  \n",
       "0  Apparel_Bottomwear_Jeans  \n",
       "1  Apparel_Bottomwear_Jeans  \n",
       "2  Apparel_Bottomwear_Jeans  \n",
       "3  Apparel_Bottomwear_Jeans  \n",
       "4  Apparel_Bottomwear_Jeans  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Apparel_Bottomwear_Jeans', 'Apparel_Bottomwear_Skirts',\n",
       "       'Apparel_Bottomwear_Track Pants', 'Apparel_Saree_Sarees',\n",
       "       'Apparel_Topwear_Shirts', 'Apparel_Topwear_Sweaters',\n",
       "       'Apparel_Topwear_Tshirts', 'Footwear_Flip Flops_Flip Flops',\n",
       "       'Footwear_Sandal_Sandals', 'Footwear_Shoes_Casual Shoes',\n",
       "       'Footwear_Shoes_Formal Shoes', 'Footwear_Shoes_Sports Shoes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_df['label'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Y2Pczez6AEWw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yw_22W13XcwV"
   },
   "outputs": [],
   "source": [
    "target_size = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(imgs_df, folder_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Preprocess images and labels without TensorFlow.\n",
    "\n",
    "    Args:\n",
    "        imgs_df: DataFrame containing image metadata (e.g., filename, category, label).\n",
    "        folder_path: Root folder containing the images.\n",
    "        target_size: Tuple specifying the target size of the image (width, height).\n",
    "\n",
    "    Returns:\n",
    "        images: Numpy array of preprocessed images.\n",
    "        labels: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for ind, row in imgs_df.iterrows():\n",
    "        try:\n",
    "            # Build the file path based on the category\n",
    "            if row['masterCategory'] == 'Apparel':\n",
    "                file_path = f\"{folder_path}/clothing/{row['filename']}\"\n",
    "            elif row['masterCategory'] == 'Footwear':\n",
    "                file_path = f\"{folder_path}/foot wear/{row['filename']}\"\n",
    "            else:\n",
    "                continue  # Skip rows with other categories\n",
    "\n",
    "            # Load the image using Pillow\n",
    "            img = Image.open(file_path).convert('RGB')\n",
    "\n",
    "            # Resize the image\n",
    "            img = img.resize(target_size)\n",
    "\n",
    "            # Convert the image to a numpy array\n",
    "            img_array = np.asarray(img, dtype=np.float32)\n",
    "\n",
    "            # Normalize the image to match ResNet preprocessing\n",
    "            img_array /= 255.0  # Scale pixel values to [0, 1]\n",
    "          \n",
    "\n",
    "            # Append the preprocessed image and label\n",
    "            images.append(img_array)\n",
    "            labels.append(row['label'])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {row['id']}: {e}\")\n",
    "\n",
    "    # Convert the list of images to a numpy array\n",
    "    images = np.array(images)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, pairs, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pairs: Array of image pairs (numpy arrays).\n",
    "            labels: Array of labels (1 for similar, 0 for dissimilar).\n",
    "            transform: Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.pairs = pairs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False  # Freeze the pretrained layers\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  # Remove the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through ResNet and flatten the output\n",
    "        x = self.feature_extractor(x)\n",
    "        return torch.flatten(x, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=2048):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, embedding1, embedding2):\n",
    "        # Compute the absolute difference between embeddings\n",
    "        diff = torch.abs(embedding1 - embedding2)\n",
    "        return self.fc(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = preprocess_images(imgs_df, folder_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3\n",
      "  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n",
      "  5  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n",
      "  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8\n",
      "  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n",
      " 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "print(f\"Encoded labels: {encoded_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_image_pairs(images, labels, target_pair_count=2000):\n",
    "    \"\"\"\n",
    "    Create at least `target_pair_count` pairs of images and their corresponding labels (1 for similar, 0 for dissimilar).\n",
    "\n",
    "    Args:\n",
    "    - images: Array of preprocessed images.\n",
    "    - labels: Array of labels corresponding to the images.\n",
    "    - target_pair_count: Total number of pairs to generate.\n",
    "\n",
    "    Returns:\n",
    "    - pairs: Array of paired images.\n",
    "    - pair_labels: Array of labels for the pairs (1: similar, 0: dissimilar).\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    unique_labels = np.unique(labels)\n",
    "    #print('here')\n",
    "    label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
    "    print(label_indices)\n",
    "    #print('here_1')\n",
    "\n",
    "    # Calculate number of positive and negative pairs\n",
    "    num_positive_pairs = target_pair_count // 2\n",
    "    num_negative_pairs = target_pair_count - num_positive_pairs\n",
    "\n",
    "    # Generate positive pairs\n",
    "    while len(pairs) < num_positive_pairs:\n",
    "        #print('h3')\n",
    "        for label, indices in label_indices.items():\n",
    "            if len(indices) < 2:\n",
    "                continue  # Skip if not enough samples for positive pairs\n",
    "            i, j = np.random.choice(indices, size=2, replace=False)\n",
    "            pairs.append([images[i], images[j]])\n",
    "            pair_labels.append(1)\n",
    "            #print('1')\n",
    "            if len(pairs) >= num_positive_pairs:\n",
    "                break\n",
    "\n",
    "    # Generate negative pairs\n",
    "    while len(pairs) < target_pair_count:\n",
    "        for label, indices in label_indices.items():\n",
    "            i = np.random.choice(indices)\n",
    "            neg_label = np.random.choice(unique_labels[unique_labels != label])\n",
    "            j = np.random.choice(label_indices[neg_label])\n",
    "            pairs.append([images[i], images[j]])\n",
    "            pair_labels.append(0)\n",
    "            #print('0')\n",
    "            if len(pairs) >= target_pair_count:\n",
    "                break\n",
    "\n",
    "    return np.array(pairs), np.array(pair_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69], dtype=int64), 1: array([ 70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
      "        83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,\n",
      "        96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
      "       109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
      "       122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134,\n",
      "       135, 136, 137, 138, 139], dtype=int64), 2: array([140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152,\n",
      "       153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
      "       166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178,\n",
      "       179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,\n",
      "       192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
      "       205, 206, 207, 208, 209], dtype=int64), 3: array([210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
      "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
      "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
      "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279], dtype=int64), 4: array([280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292,\n",
      "       293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318,\n",
      "       319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331,\n",
      "       332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344,\n",
      "       345, 346, 347, 348, 349], dtype=int64), 5: array([350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
      "       363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375,\n",
      "       376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388,\n",
      "       389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401,\n",
      "       402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414,\n",
      "       415, 416, 417, 418, 419], dtype=int64), 6: array([420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
      "       446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
      "       459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
      "       472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,\n",
      "       485, 486, 487, 488, 489], dtype=int64), 7: array([490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502,\n",
      "       503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515,\n",
      "       516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528,\n",
      "       529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541,\n",
      "       542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554,\n",
      "       555, 556, 557, 558, 559], dtype=int64), 8: array([560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572,\n",
      "       573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585,\n",
      "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598,\n",
      "       599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611,\n",
      "       612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624,\n",
      "       625, 626, 627, 628, 629], dtype=int64), 9: array([630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
      "       643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
      "       656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
      "       669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
      "       682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
      "       695, 696, 697, 698, 699], dtype=int64), 10: array([700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712,\n",
      "       713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725,\n",
      "       726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738,\n",
      "       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n",
      "       752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764,\n",
      "       765, 766, 767, 768], dtype=int64), 11: array([769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781,\n",
      "       782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794,\n",
      "       795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807,\n",
      "       808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820,\n",
      "       821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833,\n",
      "       834, 835, 836], dtype=int64)}\n",
      "Number of pairs: 4000\n",
      "Pair labels distribution: (array([0, 1]), array([2000, 2000], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Assuming `images` is a numpy array of preprocessed images and `labels` is a numpy array of their labels\n",
    "pairs, pair_labels = create_image_pairs(images, encoded_labels, target_pair_count=4000)\n",
    "print(f\"Number of pairs: {len(pairs)}\")\n",
    "print(f\"Pair labels distribution: {np.unique(pair_labels, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split pairs and labels into training and validation sets\n",
    "train_pairs, val_pairs, train_labels, val_labels = train_test_split(\n",
    "    pairs, pair_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SiameseDataset(train_pairs, train_labels, transform=transform)\n",
    "val_dataset = SiameseDataset(val_pairs, val_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ruthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize models\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "siamese_network = SiameseNetwork(embedding_dim=2048).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(siamese_network.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5617, Val Loss: 0.4902\n",
      "Epoch 2/10, Train Loss: 0.4151, Val Loss: 0.3876\n",
      "Epoch 3/10, Train Loss: 0.3446, Val Loss: 0.3460\n",
      "Epoch 4/10, Train Loss: 0.3043, Val Loss: 0.3153\n",
      "Epoch 5/10, Train Loss: 0.2741, Val Loss: 0.2884\n",
      "Epoch 6/10, Train Loss: 0.2474, Val Loss: 0.2743\n",
      "Epoch 7/10, Train Loss: 0.2302, Val Loss: 0.2676\n",
      "Epoch 8/10, Train Loss: 0.2123, Val Loss: 0.2543\n",
      "Epoch 9/10, Train Loss: 0.2007, Val Loss: 0.2473\n",
      "Epoch 10/10, Train Loss: 0.1870, Val Loss: 0.2423\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    feature_extractor.eval()  # Feature extractor in evaluation mode\n",
    "    siamese_network.train()  # Siamese network in training mode\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for img1, img2, labels in train_loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "        # Extract embeddings\n",
    "        with torch.no_grad():\n",
    "            embedding1 = feature_extractor(img1)\n",
    "            embedding2 = feature_extractor(img2)\n",
    "\n",
    "        # Forward pass through the Siamese network\n",
    "        outputs = siamese_network(embedding1, embedding2).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation Phase\n",
    "    feature_extractor.eval()\n",
    "    siamese_network.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in val_loader:\n",
    "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "\n",
    "            # Extract embeddings\n",
    "            embedding1 = feature_extractor(img1)\n",
    "            embedding2 = feature_extractor(img2)\n",
    "\n",
    "            # Forward pass through the Siamese network\n",
    "            outputs = siamese_network(embedding1, embedding2).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print the losses for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(siamese_network.state_dict(), \"siamese_network_2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
